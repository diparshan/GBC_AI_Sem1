# -*- coding: utf-8 -*-
"""Toolkit_AutoFeatureSelector.ipynb.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18WRa2MniENcobUGuZQbdu2GKhmbG6r3B
"""

import pandas as pd
from sklearn.feature_selection import SelectKBest, chi2, RFE
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from lightgbm import LGBMClassifier
from collections import Counter
import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler

# Placeholder for correlation-based selector
def cor_selector(X, y, num_feats):
    cor_list = []
    for i in X.columns:
        cor = np.corrcoef(X[i], y)[0, 1]
        cor_list.append(abs(cor))
    cor_list = np.nan_to_num(cor_list)
    feature_indices = np.argsort(cor_list)[-num_feats:]
    cor_support = [i in feature_indices for i in range(len(X.columns))]
    cor_feature = X.columns[feature_indices].tolist()
    return cor_support, cor_feature

# Chi-square selector
def chi_squared_selector(X, y, num_feats):
    chi_selector = SelectKBest(score_func=chi2, k=num_feats)
    chi_selector.fit(X, y)
    chi_support = chi_selector.get_support()
    chi_feature = X.columns[chi_support].tolist()
    return chi_support, chi_feature

# Recursive Feature Elimination (RFE)
def rfe_selector(X, y, num_feats):
    model = LogisticRegression(solver='lbfgs')
    rfe_selector = RFE(estimator=model, n_features_to_select=num_feats, step=1,verbose=5)
    rfe_selector.fit(X, y)
    rfe_support = rfe_selector.get_support()
    rfe_feature = X.columns[rfe_support].tolist()
    return rfe_support, rfe_feature

# Logistic Regression Embedded Method
def embedded_log_reg_selector(X, y, num_feats):
    model = LogisticRegression(solver='liblinear', penalty='l2', max_iter=50000)
    model.fit(X, y)
    importance = abs(model.coef_).flatten()
    feature_indices = np.argsort(importance)[-num_feats:]
    embedded_lr_support = [i in feature_indices for i in range(len(X.columns))]
    embedded_lr_feature = X.columns[feature_indices].tolist()
    return embedded_lr_support, embedded_lr_feature

# Random Forest Embedded Method
def embedded_rf_selector(X, y, num_feats):
    model = RandomForestClassifier(n_estimators=100)
    model.fit(X, y)
    importance = model.feature_importances_
    feature_indices = np.argsort(importance)[-num_feats:]
    embedded_rf_support = [i in feature_indices for i in range(len(X.columns))]
    embedded_rf_feature = X.columns[feature_indices].tolist()
    return embedded_rf_support, embedded_rf_feature

# LightGBM Embedded Method
def embedded_lgbm_selector(X, y, num_feats):
    model = LGBMClassifier(n_estimators=500,
                      learning_rate=0.05,
                      num_leaves=32,
                      colsample_bytree=0.2,
                      reg_alpha=3,
                      reg_lambda=1,
                      min_split_gain=0.01,
                      min_child_weight=40)
    model.fit(X, y)
    importance = model.feature_importances_
    feature_indices = np.argsort(importance)[-num_feats:]
    embedded_lgbm_support = [i in feature_indices for i in range(len(X.columns))]
    embedded_lgbm_feature = X.columns[feature_indices].tolist()
    return embedded_lgbm_support, embedded_lgbm_feature

# Preprocessing Function
def preprocess_dataset(dataset):

    # Drop any column that contains missing values
    dataset = dataset.dropna(axis=1)

    # Assuming the last column is the target variable
    y = dataset.iloc[:, -1]

    # Identify categorical columns and perform one-hot encoding
    X = dataset.iloc[:, :-1]
    categorical_cols = X.select_dtypes(include=['object', 'category']).columns
    X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)

    return X, y

# Auto Feature Selector
def autoFeatureSelector(dataset, methods=[],num_output_features=10):
    X, y= preprocess_dataset(dataset)
    num_feats = num_output_features

    all_features = []

    if 'pearson' in methods:
        _, cor_feature = cor_selector(X, y, num_feats)
        all_features.extend(cor_feature)

    if 'chi-square' in methods:
        _, chi_feature = chi_squared_selector(X, y, num_feats)
        all_features.extend(chi_feature)

    if 'rfe' in methods:
        _, rfe_feature = rfe_selector(X, y, num_feats)
        all_features.extend(rfe_feature)

    if 'log-reg' in methods:
        _, embedded_lr_feature = embedded_log_reg_selector(X, y, num_feats)
        all_features.extend(embedded_lr_feature)

    if 'rf' in methods:
        _, embedded_rf_feature = embedded_rf_selector(X, y, num_feats)
        all_features.extend(embedded_rf_feature)

    if 'lgbm' in methods:
        _, embedded_lgbm_feature = embedded_lgbm_selector(X, y, num_feats)
        all_features.extend(embedded_lgbm_feature)

    # Count occurrences of each feature
    from collections import Counter
    feature_votes = Counter(all_features)

    # Sort features by vote count in descending order
    sorted_features = [feature for feature, votes in feature_votes.most_common()]
    print(f'feature votes result: {feature_votes.most_common()}')
    # Return the top N features specified by num_output_features
    best_features = sorted_features[:num_output_features]

    return best_features

df = pd.read_csv("heart.csv")

df

len(df.columns)

best_features = autoFeatureSelector(df, methods=['pearson','chi-square', 'rfe', 'log-reg', 'rf', 'lgbm'],num_output_features=5)
best_features